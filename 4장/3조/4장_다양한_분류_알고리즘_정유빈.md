# 용어
- 다중 분류 : 타깃 데이터(target data)에 2개 이상의 클래스가 포함된 분류 문제. 소프트맥스 함수를 사용하여 클래스 예측
- numpy : 배열
- 소프트맥스 함수 : 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체합이 1 이 되도록 만드는 것
- 로지스틱 회귀 : 선형 방정싱을 사용한 분류 알고리즘. 선형회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여 클래스 확률 출력
- 시그모이드 함수 : 선형 방정식의 출력을 0과 1사이의 값으로 압축하여 이진 분류를 위해 사용
- 점진적 학습 :
- 확률적 경사 하강법 : 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘.   샘플을 하나씩 사용하지 않고 여러 개를 사용하면 미니배치 경사 하강법이 된다. 한 번에 전체 샘플을 사용하면 배치 경사 하강법이 된다.

# 4-1 로지스틱 회귀
: 로지스틱 회귀(Logistic Regressoin)는 이름은 회귀이지만 분류 모델이다.   
선형 회귀와 동일하게 선형 방정식을 학습한다.

> #### 시그모이드 함수에서 출력이 0.5 이면?
> 정확히 0.5일때, 사이킷런은 음성 클래스로 판단한다.

- 넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있다.
```
ex. 'A' 에서 'E'까지 5개의 원소로 이루어진 배열에서 'A'와 'C'만 골라내는 법
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

-> ['A', 'C']
```
#### 로지스틱 회귀(Logistic Regression)
- 기본적으로 반복전인 알고리즘 사용
- max_iter 매개변수에서 반복 횟수를 지정하면 기본값은 100
- 릿지 회귀와 같이 계수의 제곱을 규제(L2 규제)
- 릿지 회귀에서는 alpha 매개변수로 규제의 양 조절, alpha가 커지면 규제도 증가
- Logistic Regression에서 규제 제어하는 매개변수는 C, C는 작아질수록 규제 증가. 기본값은 1

#### 로지스틱 회귀를 사용하여 다중 분류
##### [가정, 각 샘플마다 5개의 특성을 가지고 있으며, 총 7개의 샘플이 있다.]
- 5개의 특성을 사용하므로, coef_ 배열의 열은 5 이다
- intercept_ 는 7 이다. 즉, 다중분류는 클래스마다 z값을 하나씩 계산하며 가장 높은 z값을 출력하는 클래스가 에측 클래스이다.

> #### 소프트맥스 함수
> 시그모이드 함수는 하나의 선형 방정식의 출력값을 0 ~ 1 사이로 압축한다. 이와 달리 소프트맥스 함수는 여러 개의 선형 방정식의 출력값을 0 ~ 1 사이로 압축하고   
> 전체 합이 1이 되도록 만든다. 이를 위해 지수 함수를 사용하기 때문에 정규화된 지수 함수라고도 부른다.

# 4-2 확률적 경사 하강법
: 
- 점진적 학습(온라인 학습) : 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 더 훈련

  



