# 용어
- 다중 분류 : 타깃 데이터(target data)에 2개 이상의 클래스가 포함된 분류 문제. 소프트맥스 함수를 사용하여 클래스 예측
- numpy : 배열
- 소프트맥스 함수 : 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체합이 1 이 되도록 만드는 것
- 로지스틱 회귀 : 선형 방정싱을 사용한 분류 알고리즘. 선형회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여 클래스 확률 출력
- 시그모이드 함수 : 선형 방정식의 출력을 0과 1사이의 값으로 압축하여 이진 분류를 위해 사용
- 점진적 학습(온라인 학습) : 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 더 훈련
- 확률적 경사 하강법 : 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘.   샘플을 하나씩 사용하지 않고 여러 개를 사용하면 미니배치 경사 하강법이 된다. 한 번에 전체 샘플을 사용하면 배치 경사 하강법이 된다.

# 4-1 로지스틱 회귀
: 로지스틱 회귀(Logistic Regressoin)는 이름은 회귀이지만 분류 모델이다.   
선형 회귀와 동일하게 선형 방정식을 학습한다.

> #### 시그모이드 함수에서 출력이 0.5 이면?
> 정확히 0.5일때, 사이킷런은 음성 클래스로 판단한다.

- 넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있다.
```
ex. 'A' 에서 'E'까지 5개의 원소로 이루어진 배열에서 'A'와 'C'만 골라내는 법
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

-> ['A', 'C']
```
#### 로지스틱 회귀(Logistic Regression)
- 기본적으로 반복전인 알고리즘 사용
- max_iter 매개변수에서 반복 횟수를 지정하면 기본값은 100
- 릿지 회귀와 같이 계수의 제곱을 규제(L2 규제)
- 릿지 회귀에서는 alpha 매개변수로 규제의 양 조절, alpha가 커지면 규제도 증가
- Logistic Regression에서 규제 제어하는 매개변수는 C, C는 작아질수록 규제 증가. 기본값은 1

#### 로지스틱 회귀를 사용하여 다중 분류
##### [가정, 각 샘플마다 5개의 특성을 가지고 있으며, 총 7개의 샘플이 있다.]
- 5개의 특성을 사용하므로, coef_ 배열의 열은 5 이다
- intercept_ 는 7 이다. 즉, 다중분류는 클래스마다 z값을 하나씩 계산하며 가장 높은 z값을 출력하는 클래스가 에측 클래스이다.

> #### 소프트맥스 함수
> 시그모이드 함수는 하나의 선형 방정식의 출력값을 0 ~ 1 사이로 압축한다. 이와 달리 소프트맥스 함수는 여러 개의 선형 방정식의 출력값을 0 ~ 1 사이로 압축하고   
> 전체 합이 1이 되도록 만든다. 이를 위해 지수 함수를 사용하기 때문에 정규화된 지수 함수라고도 부른다.

# 4-2 확률적 경사 하강법
: 가장 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표이다.   
하지만, step이 길다면 경사를 따라 내려가지 못하고 오히려 올라갈 수 있다.
<img src = "https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/3419c4e7-db35-4201-b1f9-735ecf2fdd04" width="90%"></img>
#### 가장 가파른 길을 조금씩 내려오는 것이 중요하다  

- 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정을 에포크(epoch) 라고 부른다.
- 일반적으로 경사 하강법은 수십, 수백 번 이상의 에포크를 수행한다.

#### 미니배치 경사 하강법
: 여러개의 샘플을 사용해 경사 하강법을 수행하는 방식

#### 배치 경사 하강법
: 한 번에 전체 샘플을 사용  
전체 데이터를 사용하기 때문에 가장 안정적인 방법. 하지만, 전체 데이터를 사용하면 그만큼 컴퓨터 자원을 많이 사용한다.

> #### 확률적 경사 하강법과 신경망 알고리즘
> 신경망은 일반적으로 많은 데이터를 사용하기 때문에 한 번에 모든 데이터를 사용하기 어렵다. 또 모델이 매우 복잡하기 때문에  
> 수학적인 방법으로 해답을 얻기 어렵다. 신경망 모델은 확률적 경사 하강법이나 미니배치 경사 하강법을 꼭 사용한다.

> #### 손실 함수와 비용 함수
> 비용 함수(cost function)는 손실 함수의 다른 말이다. 엄밀히 말하면 손실 함수는 샘플 하나에 대한 손실을 정의하고
> 비용 함수는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말한다. 
  



